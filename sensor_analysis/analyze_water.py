import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve, make_scorer, f1_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import time

# =========================================================
# 0. Cáº¤U HÃŒNH VÃ€ THIáº¾T Láº¬P BAN Äáº¦U
# =========================================================
RANDOM_STATE = 42
TEST_SIZE = 0.2
CONTAMINATION_RATE = 0.05

SELECTED_FEATURES = ["ph", "Turbidity", "Solids"]

print("=" * 70)
print("ğŸš€ Báº®T Äáº¦U XÃ‚Y Dá»°NG MÃ” HÃŒNH AI CHUáº¨N ÄOÃN CHáº¤T LÆ¯á»¢NG NÆ¯á»šC")
print("=" * 70)

# =========================================================
# GIAI ÄOáº N 1: TIá»€N Xá»¬ LÃ Dá»® LIá»†U
# =========================================================

print("\n" + "=" * 70)
print("ğŸ“Š GIAI ÄOáº N 1: TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
print("=" * 70)

# 1. Äá»c dataset
try:
    df = pd.read_csv("water_potability.csv")
    print(f"âœ… ÄÃ£ Ä‘á»c dataset thÃ nh cÃ´ng!")
    print(f"   ğŸ“ KÃ­ch thÆ°á»›c: {df.shape[0]} dÃ²ng x {df.shape[1]} cá»™t")
    print(f"   ğŸ“‹ CÃ¡c cá»™t: {list(df.columns)}")
except FileNotFoundError:
    print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file 'water_potability.csv'")
    exit()

# 2. Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u
print(f"\nğŸ” Kiá»ƒm tra giÃ¡ trá»‹ thiáº¿u:")
missing_before = df[SELECTED_FEATURES].isnull().sum()
print(missing_before)

for col in SELECTED_FEATURES:
    if df[col].isnull().any():
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)
        print(f"âœ… ÄÃ£ Ä‘iá»n {col}: {missing_before[col]} giÃ¡ trá»‹ thiáº¿u báº±ng median = {median_val:.2f}")

print(f"\nğŸ“Š KÃ­ch thÆ°á»›c sau xá»­ lÃ½: {df.shape}")

# Äá»‹nh nghÄ©a X vÃ  y
X = df[SELECTED_FEATURES]
y = df["Potability"]

print(f"\nğŸ“ˆ Thá»‘ng kÃª phÃ¢n bá»‘ nhÃ£n:")
print(y.value_counts())
print(f"\n   Tá»· lá»‡: {y.value_counts(normalize=True).to_dict()}")

# 3. Chia táº­p dá»¯ liá»‡u
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

print(f"\nâœ‚ï¸  ÄÃ£ chia dá»¯ liá»‡u:")
print(f"   ğŸŸ¦ Táº­p huáº¥n luyá»‡n: {X_train.shape[0]} máº«u ({(1-TEST_SIZE)*100:.0f}%)")
print(f"   ğŸŸ© Táº­p kiá»ƒm tra: {X_test.shape[0]} máº«u ({TEST_SIZE*100:.0f}%)")
print(f"   âš–ï¸  CÃ¢n báº±ng train: Potability=1 chiáº¿m {y_train.mean()*100:.1f}%")
print(f"   âš–ï¸  CÃ¢n báº±ng test: Potability=1 chiáº¿m {y_test.mean()*100:.1f}%")

# =========================================================
# GIAI ÄOáº N 2: HUáº¤N LUYá»†N VÃ€ Tá»I Æ¯U HÃ“A MÃ” HÃŒNH
# =========================================================

print("\n" + "=" * 70)
print("ğŸ¤– GIAI ÄOáº N 2: HUáº¤N LUYá»†N VÃ€ Tá»I Æ¯U HÃ“A MÃ” HÃŒNH")
print("=" * 70)

# 4. XÃ¢y dá»±ng Pipeline
clf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced'))
])

print("\nğŸ”§ ÄÃ£ táº¡o Pipeline:")
print("   1ï¸âƒ£  StandardScaler (chuáº©n hÃ³a dá»¯ liá»‡u)")
print("   2ï¸âƒ£  RandomForestClassifier (phÃ¢n loáº¡i)")

# 5. Tá»‘i Æ°u hÃ³a siÃªu tham sá»‘
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_leaf': [1, 2, 4]
}

print(f"\nâš™ï¸  Cáº¥u hÃ¬nh GridSearchCV:")
print(f"   ğŸŒ³ Sá»‘ lÆ°á»£ng cÃ¢y (n_estimators): {param_grid['classifier__n_estimators']}")
print(f"   ğŸ“ Äá»™ sÃ¢u cÃ¢y (max_depth): {param_grid['classifier__max_depth']}")
print(f"   ğŸƒ Min samples/leaf: {param_grid['classifier__min_samples_leaf']}")
print(f"   ğŸ“Š Tá»•ng sá»‘ káº¿t há»£p: {len(param_grid['classifier__n_estimators']) * len(param_grid['classifier__max_depth']) * len(param_grid['classifier__min_samples_leaf'])}")

scorer = make_scorer(f1_score, average='weighted')

grid_search = GridSearchCV(
    clf_pipeline,
    param_grid,
    cv=5,
    scoring=scorer,
    n_jobs=-1,
    verbose=2  # TÄƒng verbose Ä‘á»ƒ xem chi tiáº¿t hÆ¡n
)

print(f"\nğŸ”„ Báº¯t Ä‘áº§u GridSearchCV (5-fold cross-validation)...")
print("   â³ Äang tÃ¬m kiáº¿m siÃªu tham sá»‘ tá»‘t nháº¥t...")
start_time = time.time()

grid_search.fit(X_train, y_train)

elapsed_time = time.time() - start_time
print(f"\nâœ… HoÃ n thÃ nh GridSearchCV trong {elapsed_time:.2f} giÃ¢y!")

best_clf_model = grid_search.best_estimator_
print(f"\nğŸ† Káº¾T QUáº¢ Tá»I Æ¯U:")
print(f"   ğŸ¯ SiÃªu tham sá»‘ tá»‘t nháº¥t:")
for param, value in grid_search.best_params_.items():
    print(f"      - {param.replace('classifier__', '')}: {value}")
print(f"   ğŸ“Š F1-score (CV): {grid_search.best_score_:.4f}")

# 6. ÄÃ¡nh giÃ¡ trÃªn táº­p kiá»ƒm tra
print("\n" + "=" * 70)
print("ğŸ“ˆ ÄÃNH GIÃ MÃ” HÃŒNH TRÃŠN Táº¬P KIá»‚M TRA")
print("=" * 70)

y_pred = best_clf_model.predict(X_test)
y_pred_proba = best_clf_model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"\nğŸ¯ Káº¾T QUáº¢ Tá»”NG QUAN:")
print(f"   âœ“ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   âœ“ ROC AUC Score: {roc_auc:.4f}")

print(f"\nğŸ“Š Ma tráº­n nháº§m láº«n:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

print(f"\nğŸ“‹ BÃ¡o cÃ¡o chi tiáº¿t:")
print(classification_report(y_test, y_pred, target_names=["KhÃ´ng uá»‘ng Ä‘Æ°á»£c", "CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c"]))

# 7. Feature Importance - Sá»¬A Lá»–I: TÃ­nh toÃ¡n TRÆ¯á»šC KHI váº½
fitted_scaler = best_clf_model.named_steps['scaler']
fitted_classifier = best_clf_model.named_steps['classifier']

feature_importances = pd.Series(
    fitted_classifier.feature_importances_,
    index=SELECTED_FEATURES
).sort_values(ascending=False)

print(f"\nğŸ” PHÃ‚N TÃCH Äáº¶C TRÆ¯NG QUAN TRá»ŒNG:")
for feature, importance in feature_importances.items():
    print(f"   {'â–ˆ' * int(importance * 50)} {feature}: {importance:.4f}")

# =========================================================
# Váº¼ CÃC BIá»‚U Äá»’
# =========================================================

print(f"\nğŸ“Š Äang táº¡o biá»ƒu Ä‘á»“...")

# Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
            xticklabels=["KhÃ´ng uá»‘ng Ä‘Æ°á»£c", "CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c"],
            yticklabels=["KhÃ´ng uá»‘ng Ä‘Æ°á»£c", "CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c"])
plt.xlabel("Dá»± Ä‘oÃ¡n")
plt.ylabel("Thá»±c táº¿")
plt.title(f"Ma tráº­n nháº§m láº«n\nAccuracy: {accuracy:.2%}")
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
print("   âœ… ÄÃ£ lÆ°u: confusion_matrix.png")
plt.show()

# Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances.values, y=feature_importances.index, palette="viridis")
plt.xlabel("Má»©c Ä‘á»™ quan trá»ng")
plt.ylabel("Äáº·c trÆ°ng")
plt.title("Feature Importance cá»§a Random Forest")
for i, v in enumerate(feature_importances.values):
    plt.text(v + 0.01, i, f'{v:.3f}', va='center')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
print("   âœ… ÄÃ£ lÆ°u: feature_importance.png")
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.3f})', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Ngáº«u nhiÃªn (AUC = 0.50)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ÄÆ°á»ng cong ROC')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')
print("   âœ… ÄÃ£ lÆ°u: roc_curve.png")
plt.show()

# Biá»ƒu Ä‘á»“ 4: PhÃ¢n bá»‘ xÃ¡c suáº¥t dá»± Ä‘oÃ¡n (Probability Distribution)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, label='KhÃ´ng uá»‘ng Ä‘Æ°á»£c (thá»±c táº¿)', color='red', edgecolor='black')
plt.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, label='CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c (thá»±c táº¿)', color='green', edgecolor='black')
plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='NgÆ°á»¡ng quyáº¿t Ä‘á»‹nh (0.5)')
plt.xlabel('XÃ¡c suáº¥t dá»± Ä‘oÃ¡n "CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c"')
plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')
plt.title('PhÃ¢n bá»‘ xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('probability_distribution.png', dpi=300, bbox_inches='tight')
print("   âœ… ÄÃ£ lÆ°u: probability_distribution.png")
plt.show()

# Biá»ƒu Ä‘á»“ 5: PhÃ¢n bá»‘ dá»¯ liá»‡u theo tá»«ng Ä‘áº·c trÆ°ng (Feature Distribution)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
colors = ['#FF6B6B', '#4ECDC4']
for idx, feature in enumerate(SELECTED_FEATURES):
    ax = axes[idx]
    
    # Váº½ histogram cho 2 lá»›p
    ax.hist(X_test[y_test == 0][feature], bins=20, alpha=0.6, 
            label='KhÃ´ng uá»‘ng Ä‘Æ°á»£c', color=colors[0], edgecolor='black')
    ax.hist(X_test[y_test == 1][feature], bins=20, alpha=0.6, 
            label='CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c', color=colors[1], edgecolor='black')
    
    ax.set_xlabel(feature, fontsize=12, fontweight='bold')
    ax.set_ylabel('Sá»‘ lÆ°á»£ng máº«u', fontsize=11)
    ax.set_title(f'PhÃ¢n bá»‘ {feature}', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('feature_distribution.png', dpi=300, bbox_inches='tight')
print("   âœ… ÄÃ£ lÆ°u: feature_distribution.png")
plt.show()

# =========================================================
# GIAI ÄOáº N 3: MÃ” HÃŒNH PHÃT HIá»†N Báº¤T THÆ¯á»œNG
# =========================================================

print("\n" + "=" * 70)
print("ğŸ” GIAI ÄOáº N 3: HUáº¤N LUYá»†N MÃ” HÃŒNH PHÃT HIá»†N Báº¤T THÆ¯á»œNG")
print("=" * 70)

anomaly_detector = IsolationForest(random_state=RANDOM_STATE, contamination=CONTAMINATION_RATE)
anomaly_detector.fit(fitted_scaler.transform(X_train))

print(f"âœ… ÄÃ£ huáº¥n luyá»‡n Isolation Forest")
print(f"   ğŸ“Š Contamination rate: {CONTAMINATION_RATE*100:.1f}%")

# =========================================================
# GIAI ÄOáº N 4: LÆ¯U MÃ” HÃŒNH
# =========================================================

print("\n" + "=" * 70)
print("ğŸ’¾ GIAI ÄOáº N 4: LÆ¯U TRá»® MÃ” HÃŒNH")
print("=" * 70)

joblib.dump(best_clf_model, 'clf_pipeline_potability.pkl')
joblib.dump(anomaly_detector, 'anomaly_detector_potability.pkl')
joblib.dump(SELECTED_FEATURES, 'selected_features.pkl')

print("âœ… ÄÃ£ lÆ°u cÃ¡c file:")
print("   ğŸ“ clf_pipeline_potability.pkl")
print("   ğŸ“ anomaly_detector_potability.pkl")
print("   ğŸ“ selected_features.pkl")

# =========================================================
# GIAI ÄOáº N 5: Dá»° ÄOÃN Dá»® LIá»†U Má»šI
# =========================================================

print("\n" + "=" * 70)
print("ğŸ”® GIAI ÄOáº N 5: Dá»° ÄOÃN Dá»® LIá»†U Má»šI")
print("=" * 70)

# Táº£i láº¡i mÃ´ hÃ¬nh
loaded_clf_pipeline = joblib.load('clf_pipeline_potability.pkl')
loaded_anomaly_detector = joblib.load('anomaly_detector_potability.pkl')
loaded_selected_features = joblib.load('selected_features.pkl')
loaded_scaler_from_pipeline = loaded_clf_pipeline.named_steps['scaler']
loaded_classifier_from_pipeline = loaded_clf_pipeline.named_steps['classifier']

print("âœ… ÄÃ£ táº£i láº¡i mÃ´ hÃ¬nh thÃ nh cÃ´ng")

def analyze_new_water_sample_detailed(new_data: dict):
    input_series = pd.Series(new_data, index=loaded_selected_features)
    input_df = pd.DataFrame([input_series])
    
    prediction = loaded_clf_pipeline.predict(input_df)[0]
    prediction_proba = loaded_clf_pipeline.predict_proba(input_df)[0][1]
    
    potability_label = "CÃ³ thá»ƒ uá»‘ng Ä‘Æ°á»£c" if prediction == 1 else "KhÃ´ng thá»ƒ uá»‘ng Ä‘Æ°á»£c"
    
    scaled_input_for_anomaly = loaded_scaler_from_pipeline.transform(input_df)
    anomaly_score = loaded_anomaly_detector.decision_function(scaled_input_for_anomaly)[0]
    anomaly_label = loaded_anomaly_detector.predict(scaled_input_for_anomaly)[0]
    
    is_anomaly = "CÃ³" if anomaly_label == -1 else "KhÃ´ng"
    
    result = {
        "Dá»¯ liá»‡u Ä‘áº§u vÃ o": new_data,
        "Dá»± Ä‘oÃ¡n Potability": potability_label,
        "XÃ¡c suáº¥t": f"{prediction_proba:.4f} ({prediction_proba*100:.2f}%)",
        "LÃ  báº¥t thÆ°á»ng": is_anomaly,
        "Äiá»ƒm báº¥t thÆ°á»ng": f"{anomaly_score:.4f}",
        "Khuyáº¿n nghá»‹": ""
    }
    
    if is_anomaly == "CÃ³":
        result["Khuyáº¿n nghá»‹"] = "âš ï¸  Cáº¢NH BÃO: Dá»¯ liá»‡u báº¥t thÆ°á»ng! Cáº§n kiá»ƒm tra ká»¹ lÆ°á»¡ng!"
    elif potability_label == "KhÃ´ng thá»ƒ uá»‘ng Ä‘Æ°á»£c":
        result["Khuyáº¿n nghá»‹"] = "âŒ KHÃ”NG AN TOÃ€N Ä‘á»ƒ uá»‘ng. Cáº§n xá»­ lÃ½."
    else:
        result["Khuyáº¿n nghá»‹"] = "âœ… NÆ°á»›c an toÃ n Ä‘á»ƒ uá»‘ng."
    
    top_features = pd.Series(
        loaded_classifier_from_pipeline.feature_importances_, 
        index=loaded_selected_features
    ).nlargest(3)
    result["Äáº·c trÆ°ng quan trá»ng"] = top_features.to_dict()
    
    return result

# CÃ¡c máº«u thá»­ nghiá»‡m
samples = [
    {"name": "Máº«u 1 (NÆ°á»›c tá»‘t)", "data": {"ph": 7.2, "Turbidity": 2.8, "Solids": 25000}},
    {"name": "Máº«u 2 (NÆ°á»›c xáº¥u)", "data": {"ph": 4.5, "Turbidity": 8.5, "Solids": 45000}},
    {"name": "Máº«u 3 (pH báº¥t thÆ°á»ng)", "data": {"ph": 11.0, "Turbidity": 3.5, "Solids": 28000}},
    {"name": "Máº«u 4 (Turbidity cao)", "data": {"ph": 7.0, "Turbidity": 15.0, "Solids": 20000}}
]

for sample in samples:
    print(f"\n{'=' * 70}")
    print(f"ğŸ§ª {sample['name']}")
    print(f"{'=' * 70}")
    result = analyze_new_water_sample_detailed(sample['data'])
    for k, v in result.items():
        if k == "Dá»¯ liá»‡u Ä‘áº§u vÃ o":
            print(f"ğŸ“¥ {k}:")
            for key, val in v.items():
                print(f"      {key}: {val}")
        elif k == "Äáº·c trÆ°ng quan trá»ng":
            print(f"ğŸ” {k}:")
            for key, val in v.items():
                print(f"      {key}: {val:.4f}")
        else:
            print(f"   {k}: {v}")

print("\n" + "=" * 70)
print("âœ… HOÃ€N THÃ€NH Táº¤T Cáº¢ CÃC GIAI ÄOáº N")
print("=" * 70)